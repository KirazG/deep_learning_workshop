{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from keras.applications.inception_v3 import InceptionV3, preprocess_input\n",
    "from keras.layers import Dense, GlobalAveragePooling2D, Input\n",
    "from keras.models import Model\n",
    "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer learning\n",
    "\n",
    "We'll explore transfer learning in this notebook. We will transfer a deep net trained on the ImageNet dataset to a \"dog or cat\" classification task. We will discuss how to augment your data if there isn't much of it, and how to prepare a pre-trained model for transfer learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants\n",
    "DATA_PATH = '/app/data'\n",
    "TRAIN_DATA = os.path.join(DATA_PATH, 'train')\n",
    "VAL_DATA = os.path.join(DATA_PATH, 'val')\n",
    "IMG_HEIGHT, IMG_WIDTH = 299, 299\n",
    "BATCH_SIZE = 32\n",
    "STEPS_PER_EPOCH = 50\n",
    "VALIDATION_STEPS = 20\n",
    "EPOCHS = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sample some images and view them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dog_path = os.path.join(TRAIN_DATA, 'dog')\n",
    "dog_pictures = os.listdir(dog_path)\n",
    "sampled_dog_image = np.random.choice(dog_pictures)\n",
    "Image.open(os.path.join(dog_path, sampled_dog_image))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_path = os.path.join(TRAIN_DATA, 'cat')\n",
    "cat_pictures = os.listdir(cat_path)\n",
    "sampled_cat_image = os.path.join(cat_path, np.random.choice(cat_pictures))\n",
    "Image.open(sampled_cat_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When training a deep net on a small amount of data, it is often beneficial to sample augmentations of the data. This effectively multiplies the amount of data you have to train on. To do this, we'll use Keras' `ImageDataGenerator` class. \n",
    "\n",
    "Note that this class isn't actually a generator, but instead follows the factory design pattern and is used to create generators.\n",
    "\n",
    "First, let's look at how this works on the cat image we previewed earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These parameters control the range of augmentation we'll use.\n",
    "generator_factory = ImageDataGenerator(\n",
    "        rotation_range=10, \n",
    "        width_shift_range=0.2,\n",
    "        height_shift_range=0.2,            \n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True,\n",
    "        fill_mode='nearest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preview_dir = 'preview'\n",
    "if not os.path.exists(preview_dir):\n",
    "    os.mkdir(preview_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = load_img(sampled_cat_image, target_size=(IMG_HEIGHT, IMG_WIDTH))\n",
    "img = np.expand_dims(img, axis=0)\n",
    "# the `flow` method creates a generator that yields batches of images\n",
    "img_generator = generator_factory.flow(img, batch_size=1, save_to_dir=preview_dir, save_format='jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_transformed_img = next(img_generator)\n",
    "sampled_transformed_img = np.squeeze(sampled_transformed_img, axis=0)\n",
    "sampled_transformed_img = array_to_img(sampled_transformed_img)\n",
    "sampled_transformed_img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a way to generate augmented images from our dataset, let's look at how we'll model this problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Transfer Learning?\n",
    "\n",
    "Transfer learning is when a model trained on one task is \"transfered\", or adjusted, to a new task. In our case, we'll be starting with a convnet pre-trained on the ImageNet dataset and transfering it to our cat/dog classification task.\n",
    "\n",
    "Starting with a pre-trained model makes training take less time and fewer examples.\n",
    "\n",
    "To do transfer learning on such a small dataset, we will freeze all of the layers in our pre-trained Inception v3 convnet so that they won't be updated. These layers already do a great job of extracting relevant features, and training them on a small dataset like this is more likely to cause harm than to help."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise: Add trainable layers to transfer learn from Inception V3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_model():\n",
    "    # Load up the pretrained Inception v3 model.\n",
    "    img = Input(shape=(IMG_HEIGHT, IMG_WIDTH, 3))  # 3 color channels for red, green, and blue\n",
    "    inception = InceptionV3(include_top=False, input_tensor=img)\n",
    "    \n",
    "    # Freezing the layers\n",
    "    # For cat/dog classifier, we'll freeze all the layers. If you have more data and/or your problem is very \n",
    "    # different from the original task, you should experiment with leaving some of the later layers unfrozen.\n",
    "    for layer in inception.layers:\n",
    "        layer.trainable = False\n",
    "        # Batch norm layers have parameters that get updated regardless of the `trainable` property state. \n",
    "        # Setting `momentum` to 1.0 prevents those updates from happening.\n",
    "        if type(layer) == 'BatchNormalization':\n",
    "            layer.momentum = 1.0\n",
    "    \n",
    "    # The last hidden layer has 8x8 spatial dimensions (downsampled from 299x299) and 2048 channels.\n",
    "    # We'll average everything spatially to get a 2048D descriptor for the entire image.\n",
    "    features = GlobalAveragePooling2D()(inception.layers[-1].output)\n",
    "    \n",
    "    # This 2048D descriptor is now the features we'll use to train a single layer.\n",
    "    # More layers can be added here if you have enough data.\n",
    "    # Include some dense layers here\n",
    "    classifier = # ... Dense layer with 2 outputs and softmax classifier\n",
    "    model = Model(inputs=img, outputs=classifier)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = setup_model()\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's set up new data generators for training and validation data that also apply the same image preprocessing function that was used to train this Inception v3 model on ImageNet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_generator(data_dir):\n",
    "    generator_factory = ImageDataGenerator(\n",
    "            rotation_range=10, \n",
    "            width_shift_range=0.2,\n",
    "            height_shift_range=0.2,                                   \n",
    "            shear_range=0.2,\n",
    "            zoom_range=0.2,\n",
    "            horizontal_flip=True,\n",
    "            fill_mode='nearest',\n",
    "            preprocessing_function=preprocess_input)\n",
    "    \n",
    "    generator = generator_factory.flow_from_directory(\n",
    "            data_dir, \n",
    "            target_size=(IMG_HEIGHT, IMG_WIDTH))\n",
    "    return generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = setup_generator(TRAIN_DATA)\n",
    "val_generator = setup_generator(VAL_DATA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_history = model.fit_generator(\n",
    "        train_generator, \n",
    "        steps_per_epoch=STEPS_PER_EPOCH,\n",
    "        epochs=EPOCHS,\n",
    "        validation_data=val_generator,\n",
    "        validation_steps=VALIDATION_STEPS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This takes roughly 10 minutes per epoch on a laptop. In practice, you are far better off using a GPU for a problem like this. For comparison, running this on a machine with a GPU takes 30 seconds per epoch. The rule of thumb here is that if you're using convolutions in your network, then you should be using a GPU if at all possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def undo_image_preprocessing(img):\n",
    "    x = img / 2.\n",
    "    x += 0.5\n",
    "    x *= 255.\n",
    "    return x\n",
    "\n",
    "def sample_image_and_label(generator):\n",
    "    \"\"\"Sample image and label\n",
    "\n",
    "    Returns both the preprocessed image data and the unprocessed image\n",
    "    to make it easy to view and run predictions.\"\"\"\n",
    "    image_batch, label_batch = next(generator)\n",
    "    test_img_data, test_label = image_batch[0], label_batch[0]\n",
    "\n",
    "    test_img = undo_image_preprocessing(test_img_data)\n",
    "    test_img = test_img.astype(np.uint8)\n",
    "    test_img = Image.fromarray(test_img, mode='RGB')\n",
    "    \n",
    "    test_img_data = np.expand_dims(test_img_data, axis=0)\n",
    "\n",
    "    return test_img_data, test_img, test_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_img_data, test_img, test_label = sample_image_and_label(val_generator)\n",
    "\n",
    "prediction = model.predict(test_img_data)\n",
    "print 'label:', np.argmax(test_label), 'prediction:', np.argmax(prediction)\n",
    "print '=' * 22\n",
    "\n",
    "test_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
